{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dyna-Q",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "QJEOPRdMVIFq",
        "3NhPmCUnXN74"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliusfrost/Research-Paper-Implementations/blob/master/Dyna_Q.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_Cdgv5QGE5b",
        "colab_type": "text"
      },
      "source": [
        "# Dyna-Q\n",
        "\n",
        "Q learning with Model based planning\n",
        "\n",
        "Works for discrete state transitions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd8aw6uBGtXL",
        "colab_type": "text"
      },
      "source": [
        "## Build Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BhOW41HLrLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJEOPRdMVIFq",
        "colab_type": "text"
      },
      "source": [
        "###define the maze environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmQ4yiSzKn9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid = [[0,0,0,0,0],\n",
        "        [0,1,0,1,0],\n",
        "        [0,1,0,0,0],\n",
        "        [0,1,0,1,0],\n",
        "        [0,0,0,1,0]]\n",
        "\n",
        "class Env:\n",
        "    def __init__(self):\n",
        "        self.grid = grid\n",
        "        self.start = (0,0)\n",
        "        self.end = (len(self.grid)-1, len(self.grid)-1)\n",
        "        self.position = self.start\n",
        "        self.goal = self.end\n",
        "        \n",
        "    def reset(self):\n",
        "        self.grid = grid\n",
        "        self.position = self.start\n",
        "        return self.get_state()\n",
        "    \n",
        "    def possible_actions(self):\n",
        "        up, down, left, right = (-1, 0), (1, 0), (0, -1), (0, 1)\n",
        "        return (up, down, left, right)\n",
        "    \n",
        "    def step(self, action):\n",
        "        self.apply_action(action)\n",
        "        reward = self.reward()\n",
        "        next_state = self.get_state()\n",
        "        done = self.is_done()\n",
        "        return reward, next_state, done\n",
        "    \n",
        "    def is_done(self):\n",
        "        return self.position == self.goal\n",
        "    \n",
        "    def apply_action(self, action):\n",
        "        if action in self.actions_avail():\n",
        "            self.position = self.next_position(action)\n",
        "    \n",
        "    def next_position(self, action):\n",
        "        return ((self.position[0] + action[0]), (self.position[1] + action[1]))\n",
        "    \n",
        "    def actions_avail(self):\n",
        "        avail = []\n",
        "        for action in self.possible_actions():\n",
        "            next_position = self.next_position(action)\n",
        "            if 0 <= next_position[0] < len(self.grid) and 0 <= next_position[1] < len(self.grid[0]):\n",
        "                if self.grid[next_position[0]][next_position[1]] == 0:\n",
        "                    avail.append(action)\n",
        "        return avail\n",
        "        \n",
        "    def random_action(self):\n",
        "        avail = self.actions_avail()\n",
        "        i = random.randint(0, len(avail)-1)\n",
        "        return avail[i]\n",
        "        \n",
        "    def get_state(self):\n",
        "        return self.position\n",
        "        \n",
        "    def reward(self):\n",
        "        if self.position == self.goal:\n",
        "            return 10\n",
        "        else:\n",
        "            return -1\n",
        "        \n",
        "    def render(self):\n",
        "        grid_show = copy.deepcopy(self.grid)\n",
        "        grid_show[self.position[0]][self.position[1]] = 2\n",
        "        grid_show[self.goal[0]][self.goal[1]] = 3\n",
        "        print('Maze:')\n",
        "        for row in grid_show:\n",
        "            print(row)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NhPmCUnXN74",
        "colab_type": "text"
      },
      "source": [
        "###try random actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il2_9eKPVTi1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f13bf24e-97b0-4b04-e9bf-123453a076be"
      },
      "source": [
        "env = Env()\n",
        "env.reset()\n",
        "env.render()\n",
        "for i in range(10):\n",
        "    env.step(env.random_action())\n",
        "    env.render()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maze:\n",
            "[2, 0, 0, 0, 0]\n",
            "[0, 1, 0, 1, 0]\n",
            "[0, 1, 0, 0, 0]\n",
            "[0, 1, 0, 1, 0]\n",
            "[0, 0, 0, 1, 3]\n",
            "Maze:\n",
            "[0, 2, 0, 0, 0]\n",
            "[0, 1, 0, 1, 0]\n",
            "[0, 1, 0, 0, 0]\n",
            "[0, 1, 0, 1, 0]\n",
            "[0, 0, 0, 1, 3]\n",
            "Maze:\n",
            "[0, 0, 2, 0, 0]\n",
            "[0, 1, 0, 1, 0]\n",
            "[0, 1, 0, 0, 0]\n",
            "[0, 1, 0, 1, 0]\n",
            "[0, 0, 0, 1, 3]\n",
            "Maze:\n",
            "[0, 0, 0, 0, 0]\n",
            "[0, 1, 2, 1, 0]\n",
            "[0, 1, 0, 0, 0]\n",
            "[0, 1, 0, 1, 0]\n",
            "[0, 0, 0, 1, 3]\n",
            "Maze:\n",
            "[0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 1, 0]\n",
            "[0, 1, 2, 0, 0]\n",
            "[0, 1, 0, 1, 0]\n",
            "[0, 0, 0, 1, 3]\n",
            "Maze:\n",
            "[0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 1, 0]\n",
            "[0, 1, 0, 2, 0]\n",
            "[0, 1, 0, 1, 0]\n",
            "[0, 0, 0, 1, 3]\n",
            "Maze:\n",
            "[0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 1, 0]\n",
            "[0, 1, 0, 0, 2]\n",
            "[0, 1, 0, 1, 0]\n",
            "[0, 0, 0, 1, 3]\n",
            "Maze:\n",
            "[0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 1, 0]\n",
            "[0, 1, 0, 0, 0]\n",
            "[0, 1, 0, 1, 2]\n",
            "[0, 0, 0, 1, 3]\n",
            "Maze:\n",
            "[0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 1, 0]\n",
            "[0, 1, 0, 0, 0]\n",
            "[0, 1, 0, 1, 0]\n",
            "[0, 0, 0, 1, 3]\n",
            "Maze:\n",
            "[0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 1, 0]\n",
            "[0, 1, 0, 0, 0]\n",
            "[0, 1, 0, 1, 2]\n",
            "[0, 0, 0, 1, 3]\n",
            "Maze:\n",
            "[0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 1, 0]\n",
            "[0, 1, 0, 0, 0]\n",
            "[0, 1, 0, 1, 0]\n",
            "[0, 0, 0, 1, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "004s9DVZXWO7",
        "colab_type": "text"
      },
      "source": [
        "##Dyna-Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlW8BwUQDFaZ",
        "colab_type": "text"
      },
      "source": [
        "###Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abql7h0oDHyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# discount factor\n",
        "gamma = 0.9\n",
        "# step size\n",
        "alpha = 0.5\n",
        "# with probability epsilon choose random action\n",
        "epsilon = 0.1\n",
        "# maximum steps for episode termination\n",
        "max_steps = 100\n",
        "# planning steps to update q function based on model\n",
        "n = 10\n",
        "# number of episodes to train for\n",
        "episodes = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HnltrUtD5dg",
        "colab_type": "text"
      },
      "source": [
        "###define functions and updates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwqB6D8ZDzgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seen_states = set()\n",
        "seen_actions = defaultdict(set)\n",
        "Q_table = defaultdict(float)\n",
        "Model_table = defaultdict(lambda: (0, 0))\n",
        "\n",
        "env = Env()\n",
        "possible_actions = env.possible_actions()\n",
        "\n",
        "def Q(state, action):\n",
        "    return Q_table[(state, action)]\n",
        "\n",
        "\n",
        "def max_Q(state):\n",
        "    best_value = - math.inf\n",
        "    selected_action = None\n",
        "    for action in possible_actions:\n",
        "        value = Q(state, action) \n",
        "        if value > best_value:\n",
        "            best_value = value\n",
        "            selected_action = action\n",
        "    return selected_action, best_value\n",
        "\n",
        "def Q_update(state, action, reward, next_state):\n",
        "    _, best_value = max_Q(next_state)\n",
        "    Q_table[(state, action)] = Q_table[(state, action)] + alpha * (reward + gamma * best_value - Q_table[(state, action)])\n",
        "\n",
        "def Model(state, action):\n",
        "    return Model_table[(state, action)]\n",
        "    \n",
        "\n",
        "def Model_update(state, action, reward, next_state):\n",
        "    Model_table[(state, action)] = (reward, next_state)\n",
        "    \n",
        "\n",
        "def epsilon_greedy(state, Q):\n",
        "    if random.random() < epsilon:\n",
        "        action = random.choice(possible_actions)\n",
        "    else:\n",
        "        action, best_value = max_Q(state)\n",
        "    \n",
        "    return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzoFxXRmDJYH",
        "colab_type": "text"
      },
      "source": [
        "###Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8DhKCvwLvQD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "a60b981c-4482-49d1-fbf8-d90bcb2fe811"
      },
      "source": [
        "for i in range(episodes):\n",
        "    state = env.reset()\n",
        "    \n",
        "    # epsilon decay\n",
        "    epsilon *= 0.99\n",
        "    \n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    step = 0\n",
        "#     if i == episodes-1:\n",
        "#         env.render()\n",
        "    while not done and step < max_steps:\n",
        "        action = epsilon_greedy(state, Q)\n",
        "        reward, next_state, done = env.step(action)\n",
        "#         if i == episodes-1:\n",
        "#             env.render()\n",
        "        \n",
        "        Q_update(state, action, reward, next_state)\n",
        "        Model_update(state, action, reward, next_state)\n",
        "        \n",
        "        seen_states.add(state)\n",
        "        seen_actions[state].add(action)\n",
        "        \n",
        "        state = next_state\n",
        "        total_reward+=reward\n",
        "        step += 1\n",
        "        \n",
        "        for planning_step in range(n):\n",
        "            s = random.choice(list(seen_states))\n",
        "            a = random.choice(list(seen_actions[s]))\n",
        "            r, ns = Model(s, a)\n",
        "            Q_update(s, a, r, ns)\n",
        "    print(i, 'reward =', total_reward, 'steps =', step)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 reward = -74 steps = 85\n",
            "1 reward = -14 steps = 25\n",
            "2 reward = 0 steps = 11\n",
            "3 reward = 3 steps = 8\n",
            "4 reward = 1 steps = 10\n",
            "5 reward = 2 steps = 9\n",
            "6 reward = 1 steps = 10\n",
            "7 reward = 2 steps = 9\n",
            "8 reward = 2 steps = 9\n",
            "9 reward = 1 steps = 10\n",
            "10 reward = 3 steps = 8\n",
            "11 reward = 1 steps = 10\n",
            "12 reward = 3 steps = 8\n",
            "13 reward = 3 steps = 8\n",
            "14 reward = 0 steps = 11\n",
            "15 reward = 3 steps = 8\n",
            "16 reward = 3 steps = 8\n",
            "17 reward = 2 steps = 9\n",
            "18 reward = 2 steps = 9\n",
            "19 reward = 3 steps = 8\n",
            "20 reward = 3 steps = 8\n",
            "21 reward = 3 steps = 8\n",
            "22 reward = 1 steps = 10\n",
            "23 reward = 3 steps = 8\n",
            "24 reward = 3 steps = 8\n",
            "25 reward = 2 steps = 9\n",
            "26 reward = 3 steps = 8\n",
            "27 reward = 1 steps = 10\n",
            "28 reward = 2 steps = 9\n",
            "29 reward = 3 steps = 8\n",
            "30 reward = 2 steps = 9\n",
            "31 reward = 3 steps = 8\n",
            "32 reward = 2 steps = 9\n",
            "33 reward = 3 steps = 8\n",
            "34 reward = 3 steps = 8\n",
            "35 reward = 3 steps = 8\n",
            "36 reward = 3 steps = 8\n",
            "37 reward = 3 steps = 8\n",
            "38 reward = 3 steps = 8\n",
            "39 reward = 1 steps = 10\n",
            "40 reward = 3 steps = 8\n",
            "41 reward = 3 steps = 8\n",
            "42 reward = 2 steps = 9\n",
            "43 reward = 3 steps = 8\n",
            "44 reward = 3 steps = 8\n",
            "45 reward = 2 steps = 9\n",
            "46 reward = 3 steps = 8\n",
            "47 reward = 1 steps = 10\n",
            "48 reward = 1 steps = 10\n",
            "49 reward = 3 steps = 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xihJDlj4CxTC",
        "colab_type": "text"
      },
      "source": [
        "Possible actions. Same order shown in Q function and Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNCwC0g1CiHE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3ea8135e-7ce0-4f35-c804-d4eff3361b7c"
      },
      "source": [
        "up, down, left, right = possible_actions\n",
        "print('up',up)\n",
        "print('down',down)\n",
        "print('left',left)\n",
        "print('right',right)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "up (-1, 0)\n",
            "down (1, 0)\n",
            "left (0, -1)\n",
            "right (0, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVsW2lrKqxXp",
        "colab_type": "text"
      },
      "source": [
        "###Learned Q function\n",
        "\n",
        "Some places may still be 0 because the agent did not explore that part of the state-action space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmWZ13gPqFhO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "901efb30-856b-4517-80a9-9136e5452972"
      },
      "source": [
        "# for k,v in Q_table.items():\n",
        "#     print('state, action:', k, 'reward', v)\n",
        "# print(len(seen_states))\n",
        "value_grid = np.zeros((4, *np.array(grid).shape))\n",
        "for k, v in Q_table.items():\n",
        "    state, action = k\n",
        "    reward = v\n",
        "    value_grid[possible_actions.index(action), state[0], state[1]] = round(reward, 1)\n",
        "\n",
        "print('value grid per action')\n",
        "print(value_grid)\n",
        "\n",
        "max_value_grid = np.max(value_grid, axis=0)\n",
        "\n",
        "print('\\nMax value grid') \n",
        "print(max_value_grid)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "value grid per action\n",
            "[[[-1.4 -0.4  0.6  1.8  3.1]\n",
            "  [-1.4  0.   0.6  0.   3.1]\n",
            "  [-2.3  0.   1.8  4.6  4.6]\n",
            "  [-3.   0.   3.1  0.   6.2]\n",
            "  [-2.3 -0.4  1.8  0.   0. ]]\n",
            "\n",
            " [[-2.3 -0.4  1.8  1.8  4.6]\n",
            "  [-3.   0.   3.1  0.   6.2]\n",
            "  [-2.3  0.   1.8  4.6  8. ]\n",
            "  [-1.4  0.   0.6  0.  10. ]\n",
            "  [-1.4 -0.4  0.6  0.   0. ]]\n",
            "\n",
            " [[-1.4 -1.4 -0.4  0.6  1.8]\n",
            "  [-2.3  0.   1.8  0.   4.6]\n",
            "  [-3.   0.   3.1  3.1  4.6]\n",
            "  [-2.3  0.   1.8  0.   8. ]\n",
            "  [-1.4 -1.4 -0.4  0.   0. ]]\n",
            "\n",
            " [[-0.4  0.6  1.8  3.1  3.1]\n",
            "  [-2.3  0.   1.8  0.   4.6]\n",
            "  [-3.   0.   4.6  6.2  6.2]\n",
            "  [-2.3  0.   1.8  0.   8. ]\n",
            "  [-0.4  0.6  0.6  0.   0. ]]]\n",
            "\n",
            "Max value grid\n",
            "[[-0.4  0.6  1.8  3.1  4.6]\n",
            " [-1.4  0.   3.1  0.   6.2]\n",
            " [-2.3  0.   4.6  6.2  8. ]\n",
            " [-1.4  0.   3.1  0.  10. ]\n",
            " [-0.4  0.6  1.8  0.   0. ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbobCwNNqrtL",
        "colab_type": "text"
      },
      "source": [
        "###Learned Model\n",
        "\n",
        "Some places may still be 0 because the agent did not explore that part of the state-action space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx-_xuO4lDzc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "52dbac8e-473b-44f7-adf4-bc0687f24214"
      },
      "source": [
        "# for k,v in Model_table.items():\n",
        "#     print('state, action:', k, 'reward, next_state:', v)\n",
        "\n",
        "value_grid = np.zeros((4, *np.array(grid).shape))\n",
        "for k,v in Model_table.items():\n",
        "    state, action = k\n",
        "    reward, next_state = v\n",
        "    value_grid[possible_actions.index(action), state[0], state[1]] = reward\n",
        "\n",
        "print('value grid per action')\n",
        "print(value_grid)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "value grid per action\n",
            "[[[-1. -1. -1. -1. -1.]\n",
            "  [-1.  0. -1.  0. -1.]\n",
            "  [-1.  0. -1. -1. -1.]\n",
            "  [-1.  0. -1.  0. -1.]\n",
            "  [-1. -1. -1.  0.  0.]]\n",
            "\n",
            " [[-1. -1. -1. -1. -1.]\n",
            "  [-1.  0. -1.  0. -1.]\n",
            "  [-1.  0. -1. -1. -1.]\n",
            "  [-1.  0. -1.  0. 10.]\n",
            "  [-1. -1. -1.  0.  0.]]\n",
            "\n",
            " [[-1. -1. -1. -1. -1.]\n",
            "  [-1.  0. -1.  0. -1.]\n",
            "  [-1.  0. -1. -1. -1.]\n",
            "  [-1.  0. -1.  0. -1.]\n",
            "  [-1. -1. -1.  0.  0.]]\n",
            "\n",
            " [[-1. -1. -1. -1. -1.]\n",
            "  [-1.  0. -1.  0. -1.]\n",
            "  [-1.  0. -1. -1. -1.]\n",
            "  [-1.  0. -1.  0. -1.]\n",
            "  [-1. -1. -1.  0.  0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}